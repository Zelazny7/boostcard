{
    "contents" : "# save(mod, df, drop, s, file='test.data.rData')\nload(file='test.data.rData')\n\n### TRY A RISKVIEW MODEL QUICKLY ###\n# library(gbm)\n# library(mjollnir)\n# df <- read.csv(file = \"F:/RV5A1503_6666/data/rv5t_rdev.csv\", header=TRUE,\n#                na.strings = c('','.'), stringsAsFactors = TRUE)\n# dev <- df$Selected == 1\n# ok.cols <- grep(\"^\\\\w_\\\\w\\\\d\\\\d\", colnames(df))\n# drop <- which(!get.final.cols(df[dev, ], c(colnames(df)[-ok.cols])))\n#\n# mono <- get.mono.cols(colnames(df)[-drop])\n# s <- sample(which(dev), sum(dev)/10)\n#\n# # see what else wants to come into RV5A\n# mod <- gbm.fit(x = df[s,-drop], y = df[s, 'depvar'],\n#                distribution = 'bernoulli', var.monotone = mono,\n#                n.trees = 1000, shrinkage = 0.1, nTrain = length(s)/2,\n#                keep.data = TRUE, verbose = TRUE, interaction.depth = 1,\n#                n.minobsinnode = 100)\n\n###### END GBM BUILD #######\n\nlibrary(gbm)\nlibrary(ggplot2)\ndata(titanic, package='mjollnir')\ntitanic$Parch <- ordered(titanic$Parch)\ntitanic$Pclass <- as.numeric(titanic$Pclass)\nmono <- c(-1, 0, -1, 1, 1, 1, 0)\nmod <- gbm(Survived~., data=titanic, n.trees = 5000, verbose=T, var.monotone=NULL, n.minobsinnode = 50)\nn.trees <- 1:5000\n\n#### figure out the ordered variables ###\n\nclassing <- gbm.to.scorecard(mod, n.trees, titanic)\nout$score +  classing[[1]]\n\n\n\nlvls <- vlv[vlv <= tree[[2]][p] + 1]\n\n\nclassing1 <- gbm.to.scorecard(mod2, 1:gbm.perf(mod2), titanic, precision = 0.001)\nclassing2 <- gbm.to.scorecard(mod2, 1:gbm.perf(mod2), titanic, precision = 0.50)\nclassing3 <- gbm.to.scorecard(mod2, 1:gbm.perf(mod2), titanic, precision = 1.00)\n\nphat1 <- predict.scorecard(classing1, titanic)\nphat2 <- predict.scorecard(classing2, titanic)\nphat3 <- predict.scorecard(classing3, titanic)\n\nlibrary(mjollnir)\nks.table(phat1, titanic$Survived)$ks\nks.table(phat2, titanic$Survived)$ks\nks.table(phat3, titanic$Survived)$ks\n\n# function to turn a classed list into a plot data.frame\nconvert <- function(x) {\n  data.frame(\n    value = c('Null', x$value),\n    score = c(x$missing, x$score),\n    stringsAsFactors=F)}\n\nplots <- lapply(seq_along(classing), function(i) {\n  if (i > 1) {\n    plt <- convert(classing[[i]])\n    ggplot(plt, aes(x=value, y=score, fill=score)) +\n      geom_bar(stat='identity', color='black') +\n      coord_flip() +\n      scale_x_discrete(limits=rev(plt$value)) +\n      scale_fill_gradient2(low='blue', high='red') +\n      ggtitle(names(classing)[i])\n  }})\n\nplt <- convert(classing[[5]])\n\n# woe plot alongside a counts plot\nggplot(plt, aes(x=value, y=score, fill=score)) +\n  geom_bar(stat='identity', color='black') +\n  coord_flip() +\n  scale_x_discrete(limits=rev(plt$value), expand=c(0.05, 2)) +\n  scale_fill_gradient2(low='blue', high='red') +\n  ggtitle(names(classing)[5])\n\nggplot(plt, aes(x=value, y=score, fill=score)) +\n  geom_bar(stat='identity', color='black') +\n  coord_flip() +\n  scale_x_discrete(limits=rev(plt$value), expand=c(0.05, 2)) +\n  scale_fill_gradient2(low='blue', high='red') +\n  ggtitle(names(classing)[5])\n\n\nphat <- predict.scorecard(classing, df[s, -drop])\nphat.mod <- predict(mod, df[s, -drop], gbm.perf(mod))\n\n\n\nadj.weights <- function(x) {\n  x$missing <- 0\n  x\n}\n\nclassing2 <- c(classing[1], lapply(classing[-1], adj.weights))\n\nna.rec <- rep(NA, ncol(df[s, -drop]))\n\ntmp <- rbind(df[s, -drop], na.rec)\n\nphat <- predict.scorecard(classing, tmp)\n\nphat2 <- predict.scorecard(classing2, df[s, -drop])\n\n\n#### CREATE SOME CLUSTERS !!! ####\n\n\n##################################\n### INVERSE DOCUMENT FREQUENCY ###\n##################################\n\nfpath <- 'X:/Backup/Heather/RV5T_4752_1503/'\nfname <- 'ln_4752_rv5t_dev_val_fcra50_archive_20150303_edina_v50.csv'\n\ncc <- rep(\"NULL\", 1212)\ncc[c(134, 138)] <- \"character\"\ncc[4] <- 'factor'\n\nsources <- read.csv(paste0(fpath, fname), header=F, colClasses=cc,\n                    na.string=c(\"\",\".\"))\n\n# compute the inverse-document-frequency\nget.idf <- function(x) {\n  tokens <- strsplit(x[1], split = \",\")[[1]]\n  counts <- as.numeric(strsplit(x[2], split = \",\")[[1]])\n\n  names(counts) <- tokens\n  counts\n}\n\n# now need to reduce the list to one list of counts\nres <- apply(sources[-1], 1, get.idf)\n\n# find unique list of values\nvals <- na.omit(unique(unlist(sapply(res, names))))\nfreqs <- rep(1, length(vals))\nnames(freqs) <- vals\n\nfor (i in seq_along(res)) {\n  ids <- na.omit(names(res[[i]]))\n  freqs[ids] <- freqs[ids] + ifelse(res[[i]] > 0, 1, 0)\n}\n\nidf <- log(length(res)/freqs)\n\n##################################\n### TERM FREQUENCY PER RECORD  ###\n##################################\n\nterm.frequency <- function(x) {\n  ids <- na.omit(names(x))\n  freqs <- rep(1, length(vals))\n  names(freqs) <- vals\n  freqs[ids] <- freqs[ids] + x\n  log(freqs)\n}\n\ntf <- do.call(rbind, lapply(res, term.frequency))\n\n### MULITPLY THEM ###\ntf.idf <- t(t(tf) * idf)\n\n### Cluster them ###\nkms <- lapply(1:20, function(k) kmeans(tf.idf, k))\n\n### scree plot ###\nvar.explained <- sapply(kms, function(x) (x$totss - x$tot.withinss) / x$totss)\nplot(var.explained) # best is 10 clusters\n\ntable(kms[[10]]$cluster)\n\ndev <- df$Selected == 1\n\naggregate(df$depvar[dev], list(kms[[10]]$cluster), mean)\n\n### MERGE CLUSTER INTO DATASET ###\n\nclusters <- data.frame(cluster=kms[[10]]$cluster, account=sources[,1])\n\ndf3 <- merge(df, clusters)\n\naggregate(depvar~cluster, df3, mean)\n\n### build a GBM model on each cluster! ###\n\nok.cols <- grep(\"^\\\\w_\\\\w\\\\d\\\\d\", colnames(df3))\ndrop <- which(!get.final.cols(df3, c(colnames(df3)[-ok.cols])))\n\nmono <- get.mono.cols(colnames(df3)[-drop])\ns <- sample(nrow(df3), nrow(df3)/2)\n\nmods <- list()\nfor (k in 1:10) {\n  f <- s[df3[s, 'cluster'] == k]\n  mods[[k]] <-\n    gbm.fit(x = df3[f,-drop], y = df3[f, 'depvar'],\n            distribution = 'bernoulli', var.monotone = mono,\n            n.trees = 1000, shrinkage = 0.1, nTrain = length(f)/2,\n            keep.data = TRUE, verbose = TRUE, interaction.depth = 1,\n            n.minobsinnode = 100)\n}\n\n\n### turn them into classings ###\n\nbcs <- lapply(mods, function(x) gbm.to.scorecard(x, gbm.perf(x), df3[s,-drop]))\n\n### now predict on all da data\n\n\nphats <- lapply(bcs, predict.scorecard, df3[, -drop])\n\ncombined <- sapply(1:nrow(df3), function(i) phats[[df3$cluster[i]]][i])\n\nks.table(combined[s], df3[s, 'depvar'])\nks.table(combined[-s], df3[-s, 'depvar'])\n\n\n\n\nmod <- gbm.fit(x = df[s,-drop], y = df[s, 'depvar'],\n               distribution = 'bernoulli', var.monotone = mono,\n               n.trees = 1000, shrinkage = 0.1, nTrain = length(s)/2,\n               keep.data = TRUE, verbose = TRUE, interaction.depth = 1,\n               n.minobsinnode = 100)\n\n\n\n\n\n\n",
    "created" : 1427307610855.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "965837697",
    "id" : "8B99B106",
    "lastKnownWriteTime" : 1428959629,
    "path" : "F:/R Dev/boostcard/R/hello.R",
    "project_path" : "R/hello.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_source"
}